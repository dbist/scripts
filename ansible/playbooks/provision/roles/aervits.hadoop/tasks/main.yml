---
# tasks file for aervits.hadoop
  - name: download hadoop release
    get_url:
      url: "{{ hadoop_url }}"
      dest: "/tmp/{{ hadoop_version }}.tar.gz"
      checksum: "sha512:{{ hadoop_sha_sum }}"

  - name: create hadoop directory
    file:
      path: /opt/hadoop
      state: directory
      owner: hadoop
      group: hadoop
      mode: 0775

  - name: extract hadoop release
    unarchive:
      src: "/tmp/{{ hadoop_version }}.tar.gz"
      dest: /opt/hadoop
      remote_src: yes
      keep_newer: yes
      creates: "/opt/hadoop/{{ hadoop_version }}"

  - name: create hadoop namenode directory
    file:
      path: /dev/hadoop
      state: directory
      owner: hadoop
      group: hadoop
      mode: 0775

  - name: create datanode short-circuit path directory
    file:
      path: /var/lib/hadoop-hdfs
      state: directory
      owner: hadoop
      group: hadoop
      mode: 0755

  - name: create directory for yarn.nodemanager.recovery.dir
    file:
      path: /var/log/hadoop-yarn
      state: directory
      owner: hadoop
      group: hadoop
      mode: 0755

  - name: create yarn nodemanager local dir directory
    file:
      path: /hadoop/yarn/local
      state: directory
      owner: hadoop
      group: hadoop
      mode: 0775

  - name: configure core-site.xml
    template:
      src: templates/hadoop/core-site.xml
      dest: "/opt/hadoop/{{ hadoop_version }}/etc/hadoop/core-site.xml"

  - name: configure hdfs-site.xml
    template:
      src: templates/hadoop/hdfs-site.xml
      dest: "/opt/hadoop/{{ hadoop_version }}/etc/hadoop/hdfs-site.xml"

  - name: configure mapred-site.xml
    template:
      src: templates/hadoop/mapred-site.xml
      dest: "/opt/hadoop/{{ hadoop_version }}/etc/hadoop/mapred-site.xml"

  - name: configure yarn-site.xml
    template:
      src: templates/hadoop/yarn-site.xml
      dest: "/opt/hadoop/{{ hadoop_version }}/etc/hadoop/yarn-site.xml"

  - name: configure capacity-scheduler.xml
    template:
      src: templates/hadoop/capacity-scheduler.xml
      dest: "/opt/hadoop/{{ hadoop_version }}/etc/hadoop/capacity-scheduler.xml"

  - name: change hadoop directory permissions
    file:
      path: "/opt/hadoop/{{ hadoop_version }}"
      state: directory
      owner: hadoop
      group: hadoop
      mode: 0775
      recurse: yes

  - name: export HADOOP_HOME
    lineinfile:
      path: /etc/environment
      line: export HADOOP_HOME="/opt/hadoop/{{ hadoop_version  }}"
      state: present
      create: yes

  - name: alias hdfs command
    lineinfile:
      path: /etc/bashrc
      line: alias hdfs=$HADOOP_HOME/bin/hdfs
      state: present
      create: yes

  - name: alias hadoop command
    lineinfile:
      path: /etc/bashrc
      line: alias hadoop=$HADOOP_HOME/bin/hadoop
      state: present
      create: yes

  - name: alias yarn command
    lineinfile:
      path: /etc/bashrc
      line: alias yarn=$HADOOP_HOME/bin/yarn
      state: present
      create: yes

 # move this to users role to avoid duplication in other playbooks, i.e. hadoop, hbase
 # also not indempotent

  - name: set up passwordless ssh for service user
    shell: cat ~/.ssh/id_rsa.pub > ~/.ssh/authorized_keys
    become: true
    become_user: hadoop

  - name: format hdfs
    shell: $HADOOP_HOME/bin/hdfs namenode -format
    become: true
    become_user: hadoop
    args:
      creates: /dev/hadoop/hdfs/namenode1/current/VERSION

# fails due to pexpect module < 3.3 figure out how to install it and then uncomment start script

  - name: start namenode, datanode, secondarynamenode
    expect:
      command: /opt/hadoop/{{ hadoop_version }}/sbin/start-dfs.sh
      responses:
          'Are you sure you want to continue connecting \(yes/no\)?': 'yes'
    become: true
    become_user: hadoop

  - name: start resourcemanager, nodemanager
    expect:
      command: /opt/hadoop/{{ hadoop_version }}/sbin/start-yarn.sh
      responses:
          'Are you sure you want to continue connecting \(yes/no\)?': 'yes'
    become: true
    become_user: hadoop
